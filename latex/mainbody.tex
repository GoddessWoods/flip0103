%=================================================================
\section{Introduction}\label{sec-intro}


Emotion detection has attracted great interest 
in the natural language processing (NLP)
in the last few years.
In sentiment analysis, 
affective states are generally represented 
using either categorical or dimensional approaches.
The most popular categorical approach represents is 
Ekman’s six basic emotions 
(e.g., anger, happiness, fear, sadness, disgust and surprise)
The dimensional approach represents affective states by using
continuous numerical values in two/three dimensions,.
The valence-arousal (VA) dimensions model , 
as shown in ~\Cref{fig:va}. 
The valence represents the degree of 
positive and negative, 
while the arousal represents the degree of 
excitement and calm. 
In 1994,
Bradley and Lang proposed 
Valence- Arousal-Dominance model,
as shown in ~\Cref{fig:vad}.
And Dominance perceived degree of control in a (social) situation.
Based on this representation, 
any affective state can be represented as a point 
in the VA/VAD coordinate plane. 
Based on this representation, 
any affective state can be represented as 
a point in the VA/VAD coordinate plane.

In this paper, 
we want to predict  
multiple emotion dimension scores for an input text.
A new machine learning paradigm called 
Label Distribution Learning (LDL) 
was proposed in recently years.
Similarly,
we propose an dimensional emotion distribution learning (DEDL) algorithm
Different from the previous approaches, 
DEDL assumes that
each sentence contains a mixture of 
dimensional emotions with different intensities. 
We can label each sentence with 
an label distribution vector 
where elements corresponds to 
dimensional emotions and 
the value of each element indicates the intensity of the dimensional emotions. 
We require that each vector element has a value 
between 0 and 1 and they sum up to 1.

Use Dimensional Emotions Distribution Learning
to predict multiple emotion dimension scores for an input text.
Then use the predicted VAD and notational VAD to 
do emotion classification.
Also, 
we will use the Deep Learning to do emotion classifiction.

\begin{figure}[htbp]
	\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[height=4cm,width=6cm]{figures/va_emotion.eps}
		\caption{The emotional space spanned by the Valence-Arousal model}\label{fig:va}
	\end{minipage}%
	\hfill
	\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[height=4cm,width=6cm]{figures/six_emotions_vad.eps}
		\caption{The emotional space spanned by the Valence-Arousal-Dominance model. }\label{fig:vad}
	\end{minipage}
\end{figure}

%%==========================================================================================
%%

\section{Related Work}

\subsection{Emotion Frameworks}
\

Two main approaches of emotion representation exist, 
namely categorical approaches and dimensional approaches.
In the categorical approach, 
emotions are represented as specific discrete categories.
 Ekman’s (1992) theory of six basic emotions 
 (joy, sadness, anger, fear, disgust, and surprise) 
 is the most well-known, 
 but also Plutchik’s (1980) wheel of emotions, 
 in which joy, sadness, anger, fear, disgust, surprise, trust, 
 and anticipation are considered most basic,
is a common framework in emotion studies. 
However, 
many other theorists provide basic emotion frameworks, 
which can count up to fourteen emotion categories 
(Izard 1971; Roseman 1984).

In dimensional models, 
emotions are represented as a point in a multidimensional space. 
According to Mehrabian and Russell (1974), 
every emotional state can be described by 
scores on the dimensions valence (unhappiness-happiness), 
arousal (calmness-excitement) and dominance (submission-dominance),
known as the VAD-model. 
However, 
in later work Russell (1980) argued that 
the two dimensions valence and arousal suffice 
for describing emotional states, 
whereas Fontaine et al. (2007) suggest adding a fourth dimension: 
unpredictability.


%%==========================================================================================
%%

\subsection{Label Distribution Learning}
\

In the LDL framework, 
each instance is labeled by a real-valued vector
The goal of LDL is to 
predict multiple real-valued description degrees of the labels.
The instance variable is denoted by $ x $ , 
the particular \textit{i}-th instant is denoted by $ x_{i} $, 
the label variable is denoted by $ y $,
the particular \textit{j}-th label value is denoted by $ y_{j} $, 
the description degree of $ y $ to $ x $ is denoted by $ d ^ y_{x} $, 
and the label distribution of $ x $ is denoted by 
$  D_{i}  = \{ d^{{y_{1}} }_{ x_{i} }, d^{{y_{2}} }_{ x_{i} },...,d^{{y_{c}} }_{ x_{i} } \} $,
where $ c $ is the number of possible label values
and $ \sum_{y}  d^y_{x} = 1 $.
$ d^y_{x}  $ can be represented by 
the form of conditional probability, i.e., 
$ d^y_{x} =  P (y|x)$. 

Suppose $ p(y|x) $  is a parametric model
$ p(y|x;{\theta}) $, 
where $ \theta $ is the parameter vector. 
Given the training set $ S $, 
the goal of LDL is to find the $ /theta $ that 
can generate a distribution similar to $ D_{i} $
given the instance $ x_{i} $.
As for the Kullback-Leibler divergence,

\begin{equation}\label{eq:kl_divergence}
\theta^{\ast} =
\mathop{\arg\min}\limits_{\theta}
\sum\limits_{i}
\sum\limits_{j}
(d^{y_{j}}_{ x_{ i } }
\ln \dfrac{ d^{y_{j}}_{ x_{ i } } }{ p( y_{ j } | x_{ i } ;{\theta}) })
\end{equation}

Assumes the parametric model 
$ p(y|x;{\theta}) $ 
to be the maximum entropy model,	

\begin{equation}\label{eq:max_entropy}
	p(y|x;{\theta}) = \dfrac{1}{Z}
	\exp ( \sum\limits_{k}
	{\theta}_{y,k} 
	g_{k}( \textbf{x})  )
\end{equation}

where $ Z = \sum _{y}
\sum_{k}  {\theta}_{y,k}  g_{k}( \textbf{x}) $
is a normalization factor,
$ {\theta}_{y,k} $ is an element in \textbf {$\theta$} ,
and $ g_{k}( \textbf{x}) $ is the \textit{k}-th feature of\textbf{ x}.
Use a strategy similar to Improved Iterative Scaling (IIS) 
to find the best answer.

%%==========================================================================================
%%
%\section{Preliminaries} \label{sec-preliminaries}

\section{Data Set}

Emobank corpus is composed of 
several categories of 
the Manually Annotated Sub-Corpus of the American National Corpus 
and the corpus of SemEval-2007 Task 14 Affective Text. 
MASC is already annotated on various linguistic levels. 
SE07, on the other hand, bears annotations according to 
Ekman’s six Basic Emotion on a [0,100] scale, respectively. 
This collection of raw data comprises 10,548 sentences see in ~\Cref{tbl:emobank}.

Use the subset MASC corpus to train the DEDL model.
Then use this model to predict the scores of 
Valence, Arousal and Dominance dimensional emotion
for the sentences in the SE07 corpus.
Organize these data into two new data sets:

\begin{description}
	\item [DATA\_1] Predicted VAD is the features and 
	the emotion categories corresponding to 
	the max scales of the sentences in SE07.
	\item [DATA\_2] Annotated VAD is the features and 
	the emotion categories corresponding to 
	the max scales of the sentences in SE07.
\end{description}

\begin{table}[htbp]  \centering
	\caption{Genre distribution of the raw and filtered
		EMOBANK corpus.}
	\label{tbl:emobank}
	\begin{tabular}{cccc}
		\toprule
		Corpus & Domain  & Raw & Filtered  \\
		\midrule
		SE07 &  news headlines &  1250 &  1192 \\
		\hline
		\multirow{6}{*}{MASC} &  blogs&  1378&  1336\\
		&  essays &  1196 &  1135 \\
		&  fiction &  2893 &  2753 \\
		&  letters &  1479 &  1413 \\
		& newspapers & 1381 & 1314 \\
		& traval guides & 971 & 919 \\
		\bottomrule
	\end{tabular}
\end{table}

%%==========================================================================================
%%

\section{Method} \label{sec-method}

\subsection{Method One}
\

One sentence contain different scores of 
three dimensional emotions. 
We use $ d^y_{x} $ to indicate the intensity of 
dimensional emotion $ y $ for sentence $ x $, 
where $ x \in \chi  $ and $ y \in Y $.
The dimensional emotion intensity is needed to
meet the conditions that
$ d^y_{x} \in [0,1] $ and $\sum_{y}  d^y_{x} = 1$.
Note that $ d^y_{x} $ denotes the proportion that 
$ y $ accounts for in a valence, 
arousal  and dominance dimensional emotion distribution
of $ x $.

The training set is $ S = \{ ( x_{i} , D_{i} )  \}^{n}_{i=1} $,
where $ x_{i} \in \chi $ is a sentence embedding and
$ D_{I} = \{d^{y_{1}}_{x_{i}},  d^{y_{2}}_{x_{i}}, ... , d^{y_{c}}_{x_{i}}\} $
is the valence, arousal  and dominance dimensional emotion distribution.
The goal of DEDL is to 
learn a  conditional probability mass function $ p(y|x) $.
Assuming that $ p(y|x) $
is a parametric model $ p(y|x;{\theta}) $,
where $ \theta $ is model parameter.

Use the Kullback-Leibler divergence	as 
the distance measure,
the best parameter vector $ \theta^{\ast} $ is determined by
~\Cref{eq:kl_divergence}.

And for this problem,
the label distribution set $ D_{i} $ is that
$ D_{I} = \{d^{y_{v}}_{x_{i}},  d^{y_{a}}_{x_{i}}, d^{y_{d}}_{x_{i}}, d^{y_{n}}_{x_{i}}\} $,
where
$ d^{y_{v}}_{x_{i}} = \dfrac{score^{v}_{x_{i}}}{15} $,
$ d^{y_{v}}_{x_{i}} = \dfrac{score^{v}_{x_{i}}}{15} $,
$ d^{y_{v}}_{x_{i}} = \dfrac{score^{v}_{x_{i}}}{15} $,
and $ y_{n} = 1 - y_{v}  - y_{a} - y_{d} $.
The following we proof that 
LDL is valid for the valence, arousal  
and dominance dimensional emotion distribution.

Assume that 

\begin{equation}
T(\theta) =
\mathop{\arg\min}\limits_{\theta}
\sum\limits_{i}
\sum\limits_{j}
(d^{y_{j}}_{ x_{ i } }
\ln \dfrac{ d^{y_{j}}_{ x_{ i } } }{ p( y_{ j } | x_{ i } ;{\theta}) })
\end{equation}

for each step, 
it updates the current estimate of the parameters 
\textbf{$\theta$} to \textbf{ $\theta + \delta$},
where \textbf{$ \delta $} minimizes a lower bound to the change in likelihood




   


\section{Experiment and Analysis} \label{sec-experiment}


\begin{table} [htbp] \centering
  \caption{Precision Comparison on Event Detection Methods}
  \label{tbl:overall-experiments}
  \begin{tabular}{cccc}
\toprule
Corpus & Domain  & Raw & Filtered  \\
\midrule
SE07 &  news headlines &  1250 &  1192 \\
\hline
\multirow{6}{*}{MASC} &  blogs&  1378&  1336\\
&  essays &  1196 &  1135 \\
&  fiction &  2893 &  2753 \\
&  letters &  1479 &  1413 \\
& newspapers & 1381 & 1314 \\
& traval guides & 971 & 919 \\
\bottomrule
\end{tabular}
\end{table}


\section{Conclusions} \label{sec-conclusions}

\blindtext

\section*{Acknowledgment}

\lipsum[1]


The authors would like to thank \ldots

