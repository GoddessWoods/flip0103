%%
%% This is file `tikzposter-template.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% tikzposter.dtx  (with options: `tikzposter-template.tex')
%%
%% This is a generated file.
%%
%% Copyright (C) 2014 by Pascal Richter, Elena Botoeva, Richard Barnard, and Dirk Surmann
%%
%% This file may be distributed and/or modified under the
%% conditions of the LaTeX Project Public License, either
%% version 2.0 of this license or (at your option) any later
%% version. The latest version of this license is in:
%%
%% http://www.latex-project.org/lppl.txt
%%
%% and version 2.0 or later is part of all distributions of
%% LaTeX version 2013/12/01 or later.
%%


\documentclass{tikzposter} %Options for format can be included here

\usepackage{todonotes}

\usepackage[tikz]{bclogo}
\usepackage{lipsum}
\usepackage{amsmath}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage[absolute]{textpos}
\usepackage[it]{subfigure}
\usepackage{graphicx}
\usepackage{cmbright}
%\usepackage[default]{cantarell}
%\usepackage{avant}
%\usepackage[math]{iwona}
\usepackage[math]{kurier}
\usepackage[T1]{fontenc}


%% add your packages here
\usepackage{hyperref}
% for random text
\usepackage{lipsum}
\usepackage[english]{babel}
\usepackage[pangram]{blindtext}

\colorlet{backgroundcolor}{blue!10}

 % Title, Author, Institute
\title{Flip0103 Final Project}
\author{Shuxia Lin}
\institute{ SouthEast University, China \\
}
%\titlegraphic{logos/tulip-logo.eps}

%Choose Layout
\usetheme{Wave}

%\definebackgroundstyle{samplebackgroundstyle}{
%\draw[inner sep=0pt, line width=0pt, color=red, fill=backgroundcolor!30!black]
%(bottomleft) rectangle (topright);
%}
%
%\colorlet{backgroundcolor}{blue!10}

\begin{document}


\colorlet{blocktitlebgcolor}{blue!23}

 % Title block with title, author, logo, etc.
\maketitle

\begin{columns}
 % FIRST column
\column{0.5}% Width set relative to text width

%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%
 %\block{Main Objectives}{
%  	      	\begin{enumerate}
%  	      	\item Formalise research problem by extending \emph{outlying aspects mining}
%  	      	\item Proposed \emph{GOAM} algorithm is to solve research problem
%  	      	\item Utilise pruning strategies to reduce time complexity
%  	      	\end{enumerate}
%%  	      \end{minipage}
%}
%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%


%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%
\block{Introduction}{
	Emotion detection has attracted great interest 
	in the natural language processing (NLP)
	in the last few years.
    affective states are generally represented 
    using either categorical or dimensional approaches.
    The most popular categorical approach represents is 
    Ekman’s six basic emotions 
    (e.g., anger, happiness, fear, sadness, disgust and surprise).
    Dimensional models consider affective states to
    be best described relative to a small number of
    independent emotional dimensions 
    (often two or three): 
    Valence 
    (a positive-negative scale), 
    Arousal 
    (a calm–excited scale)
    and Dominance 
    (perceived degree of control over a situation). 
    \begin{itemize}
    	\item Use Dimensional Emotions Distribution Learning
    	to predict multiple emotion dimension scores for an input text.
    	Then use the predicted VAD and notational VAD to 
    	do emotion classification.
    	\item Use TextCNN and LSTM netural network to 
    	do emotion classification.
    \end{itemize}
}
%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%


%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%
\block{Data Set}{
Emobank corpus is composed of 
several categories of 
the Manually Annotated Sub-Corpus of the American National Corpus 
and the corpus of SemEval-2007 Task 14 Affective Text. 
MASC is already annotated on various linguistic levels. 
SE07, on the other hand, bears annotations according to 
Ekman’s six Basic Emotion on a [0,100] scale, respectively. 
Emobank contains 10,548 texts annotated with
Valence, Arousal and Dominance 
dimensional emotion scores, 
ranged from 1.0 to 5.0.

\begin{center}
	\begin{tabular}{cccc}
		\toprule
		Corpus & Domain  & Raw & Filtered  \\
		\midrule
		SE07 &  news headlines &  1250 &  1192 \\
		\hline
		&  blogs&  1378&  1336\\
		&  essays &  1196 &  1135 \\
		MASC&  fiction &  2893 &  2753 \\
		&  letters &  1479 &  1413 \\
		& newspapers & 1381 & 1314 \\
		& traval guides & 971 & 919 \\
		\bottomrule
	\end{tabular}
\end{center}
}

%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%


%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%

%\note{Note with default behavior}

%\note[targetoffsetx=12cm, targetoffsety=-1cm, angle=20, rotate=25]
%{Note \\ offset and rotated}

 % First column - second block


%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%
\block{DELD Algorithm}{
  	One sentence contain different scores of 
  	three dimensional emotions. 
  	We use $ d^y_{x} $ to indicate the intensity of 
  	dimensional emotion $ y $ for sentence $ x $, 
  	where $ x \in \chi  $ and $ y \in Y $.
  	The dimensional emotion intensity is needed to
  	meet the conditions that
  	$ d^y_{x} \in [0,1] $ and $\sum_{y}  d^y_{x} = 1$.
  	
  	The training set is $ S = \{ ( x_{i} , D_{i} )  \}^{n}_{i=1} $,
  	the label distribution set $ D_{i} $ is that
  	$ D_{I} = \{d^{y_{v}}_{x_{i}},  d^{y_{a}}_{x_{i}}, d^{y_{d}}_{x_{i}}, d^{y_{n}}_{x_{i}}\} $,
  	where
  	$ d^{y_{v}}_{x_{i}} = \dfrac{score^{v}_{x_{i}}}{15} $,
  	$ d^{y_{v}}_{x_{i}} = \dfrac{score^{v}_{x_{i}}}{15} $,
  	$ d^{y_{v}}_{x_{i}} = \dfrac{score^{v}_{x_{i}}}{15} $,
  	and $ y_{n} = 1 - y_{v}  - y_{a} - y_{d} $.
  	The goal of DEDL is to 
  	learn a  conditional probability mass function $ p(y|x) $.
  	Assuming that $ p(y|x) $
  	is a parametric model $ p(y|x;{\theta}) $,
  	where $ \theta $ is model parameter.
  	
  	Use the Kullback-Leibler divergence	as 
  	the distance measure,
  	the best parameter vector $ \theta^{\ast} $ is determined by
  	
  	\begin{equation}\label{eq:kl_divergence}
  	\theta^{\ast} =
  	\mathop{\arg\min}\limits_{\theta}
  	\sum\limits_{i}
  	\sum\limits_{j}
  	(d^{y_{j}}_{ x_{ i } }
  	\ln \dfrac{ d^{y_{j}}_{ x_{ i } } }{ p( y_{ j } | x_{ i } ;{\theta}) })
  	\end{equation}
  	
  	Assumes the parametric model 
  	$ p(y|x;{\theta}) $ 
  	to be the maximum entropy model	
  	\begin{equation}
  		 p(y|x;{\theta}) = \dfrac{1}{Z}
  		\exp ( \sum\limits_{k}
  		{\theta}_{y,k} 
  		g_{k}( \textbf{x})  ) 
  	\end{equation}
  	
  	where $ Z = \sum _{y}
  	\sum_{k}  {\theta}_{y,k}  g_{k}( \textbf{x}) $
  	is a normalization factor,
  	$ {\theta}_{y,k} $ is an element in \textbf {$\theta$} ,
  	and $ g_{k}( \textbf{x}) $ is the \textit{k}-th feature of \textbf{ x }
  
\
}

\block{TextCNN}{
	The textCNN model mainly uses 
	a one-dimensional convolutional layer and 
	a max-over-time pooling layer. 
}
%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%


% SECOND column
\column{0.5}
 %Second column with first block's top edge aligned with with previous column's top.

%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%
\block{TextCNN}{
Assume that 
the input text sequence consists of $ n $ words, 
and each word is represented by 
a $ d  $dimensional word vector. 
Then the width of the input sample is$  n $, 
the height is 1, 
and the number of input channels is $ d $. 
The calculation of textCNN is mainly divided into the following steps.

\begin{itemize}
	\item 
	Define multiple one-dimensional convolution kernels, 
	and use these convolution kernels to 
	perform convolution calculations on the inputs. 
	\item 
	All the channels of the output are individually pooled 
	in max-over-time pooling, 
	and the pooled output values of 
	these channels are connected into a vector.
	\item 
	The fully connected layer transforms 
	the connected vectors into outputs for each class. 
	This step can use the dropout layer to deal with overfitting.
\end{itemize}
}
%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%
% Second column - first block
\block{LSTM}{
	Each word first obtains a feature vector 
	from the embedding layer. 
	Then, 
	we further encode the feature sequence using 
	a bidirectional recurrent neural network to 
	obtain sequence information. 
	Finally, 
	we transform 
	the encoded sequence information to 
	output through the fully connected layer. 
	In the code I implement 
	\begin{itemize}
		\item 
		the Embedding instance is the embedding layer
		\item 
		the LSTM instance is the hidden layer of sequence encoding
		\item 
		the Linear instance is the output layer that generates the classification results
	\end{itemize}
}

%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%
\block[titleleft]{Experiment}
{
\begin{itemize}
	\item 
	Experiment One
	\begin{itemize}
		\item 
		Use Bert to train the sentence encoding/embedding
		of Emobank corpus, that is the feature vectors $ x_{i} $
		\item 
		Transform the scores of Valence, Arousal and Dominance 
		dimensional emotion into 
		the label distribution 
		$ D_{i} $ = \{ 
		$ d ˆ {y_{v}} _{ x_{i} } , 
		d ˆ {y_{a}} _{ x_{i} } ,
		d ˆ {y_{d}} _{ x_{i} } ,
		d ˆ {y_{n}} _{ x_{i} }  $ \}
		\item 
		Predict the scores of 
		Valence, Arousal and Dominance dimensional emotion
		through the EDLD
		for the sentences in the SE07 corpus.
		\item 
		Algorithm : DecisionTree, KNeighbors, LogisticRegression and so on.
	\end{itemize}
	
	\item
	Experiment Two
	\begin{itemize}
		
		\item 
		Load data
		\item 
		Text Preprocessing
		\item 
		Using a TextCNN or LSTM Model
		\item 
		Training and Evaluating the Model
		
	\end{itemize}
\end{itemize}

}
%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%

\block{	Result}{
	The result of the experiment is showed in the following table:
\begin{center}
	\begin{tabular}{cccccc}
		\toprule
		& RandomForest  & Bagging & Boosting & KNN & SVC\\
		\midrule
		DATA\_1&  0.32 &  0.30 &  0.30 & 0.31 & 0.33\\
		DATA\_2 &  0.35 &  0.34 &  0.34 & 0.30 & 0.34\\
		\bottomrule
	\end{tabular}
\end{center}

\begin{center}
	\begin{tabular}{ccccc}
		\toprule
		& acc\_train  & acc\_test & loss\_train & loss\_test \\
		\midrule
		TextCNN&  0.64 &  0.33 &  1.24 & 1.67 \\
		LSTM &  0.28 &  0.22 &  1.76 & 1.78 \\
		\bottomrule
	\end{tabular}
\end{center}

\
}
% Second column - second block
%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%
\block[titlewidthscale=1, bodywidthscale=1]
{Conclusion}
{
\begin{itemize}
	\item
	There is not much difference between 
	the values of Valence, Arousal and Dominance 
	dimensional emotions scores predicted by EDLD
	and the labeled VAD.
	\item
	Neural network parameters need to be adjusted
\end{itemize}

}
%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%


% Bottomblock
%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%
\colorlet{notebgcolor}{blue!20}
\colorlet{notefrcolor}{blue!20}
\note[targetoffsetx=8cm, targetoffsety=-4cm, angle=30, rotate=15,
radius=2cm, width=.26\textwidth]{
Acknowledgement
\begin{itemize}
    \item
    ai
 \end{itemize}
}

%\note[targetoffsetx=8cm, targetoffsety=-10cm,rotate=0,angle=180,radius=8cm,width=.46\textwidth,innersep=.1cm]{
%Acknowledgement
%}

%\block[titlewidthscale=0.9, bodywidthscale=0.9]
%{Acknowledgement}{
%}
%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%

\end{columns}


%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%
%[titleleft, titleoffsetx=2em, titleoffsety=1em, bodyoffsetx=2em,%
%roundedcorners=10, linewidth=0mm, titlewidthscale=0.7,%
%bodywidthscale=0.9, titlecenter]

%\colorlet{noteframecolor}{blue!20}
\colorlet{notebgcolor}{blue!20}
\colorlet{notefrcolor}{blue!20}
\note[targetoffsetx=-13cm, targetoffsety=-12cm,rotate=0,angle=180,radius=8cm,width=.96\textwidth,innersep=.4cm]
{
\begin{minipage}{0.3\linewidth}
\centering
\includegraphics[width=24cm]{logos/tulip-wordmark.eps}
\end{minipage}
\begin{minipage}{0.7\linewidth}
{ \centering
 Tulip
}
\end{minipage}
}
%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%


\end{document}

%\endinput
%%
%% End of file `tikzposter-template.tex'.
