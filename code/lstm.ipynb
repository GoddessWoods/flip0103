{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import random\n",
    "import tarfile\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchtext.vocab as Vocab\n",
    "import torch.utils.data as Data\n",
    "import csv\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from collections import namedtuple\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchtext\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torchtext.vocab as torchvocab\n",
    "from torch.autograd import Variable\n",
    "import tqdm\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import gensim\n",
    "import time\n",
    "import random\n",
    "import snowballstemmer\n",
    "import collections\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "sys.path.append(\"..\") \n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "DATA_ROOT = \"/Users/shuxialin/Documents/project/code\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = os.path.join(DATA_ROOT, \"aclImdb_v1.tar.gz\")\n",
    "if not os.path.exists(os.path.join(DATA_ROOT, \"aclImdb\")):\n",
    "    print(\"从压缩包解压...\")\n",
    "    with tarfile.open(fname, 'r') as f:\n",
    "        f.extractall(DATA_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "lines = []\n",
    "with open(\"/Users/shuxialin/Documents/flip/01and03/data/se07_test.xml\", 'r') as fileobj :\n",
    "    for line in fileobj :\n",
    "        lines.append(line)\n",
    "        \n",
    "lines = lines[1:]\n",
    "\n",
    "for line in lines :\n",
    "    tokens = (line.split('<')[1]).split('>')\n",
    "    sentences.append(tokens[-1])\n",
    "\n",
    "sen_se07 = sentences[:1000]\n",
    "\n",
    "se07_classes = []\n",
    "\n",
    "with open('/Users/shuxialin/Documents/flip/01and03/data/se07_test.emotions.gold', 'r') as flie :\n",
    "    for line in flie:\n",
    "        token = (line.split())[1:]\n",
    "        a = token.index(max(token))\n",
    "        se07_classes.append(a)\n",
    "        \n",
    "train_data = []\n",
    "test_data = [] \n",
    "\n",
    "for i in range(800):\n",
    "    train_data.append([sen_se07[i],se07_classes[i]])\n",
    "for i in range(200):\n",
    "    test_data.append([sen_se07[i+800],se07_classes[i+800]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_imdb(data):\n",
    "    \"\"\"\n",
    "    data: list of [string, label]\n",
    "    \"\"\"\n",
    "    def tokenizer(text):\n",
    "        return [tok.lower() for tok in text.split(' ')]\n",
    "    return [tokenizer(review) for review, _   in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('# words in vocab:', 812)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_vocab_imdb(data):\n",
    "    tokenized_data = get_tokenized_imdb(data)\n",
    "    counter = collections.Counter([tk for st in tokenized_data for tk in st])\n",
    "    return Vocab.Vocab(counter, min_freq=2)\n",
    "\n",
    "vocab = get_vocab_imdb(train_data)\n",
    "'# words in vocab:', len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_imdb(data, vocab):\n",
    "    max_l = 100  # 将每条评论通过截断或者补0，使得长度变成500\n",
    "\n",
    "    def pad(x):\n",
    "        return x[:max_l] if len(x) > max_l else x + [0] * (max_l - len(x))\n",
    "\n",
    "    tokenized_data = get_tokenized_imdb(data)\n",
    "    features = torch.tensor([pad([vocab.stoi[word] for word in words]) for words in tokenized_data])\n",
    "    labels = torch.tensor([score for _, score in data])\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "preprocess_imdb(train_data, vocab)\n",
    "train_set = Data.TensorDataset(*preprocess_imdb(train_data, vocab))\n",
    "test_set = Data.TensorDataset(*preprocess_imdb(test_data, vocab))\n",
    "train_iter = Data.DataLoader(train_set, batch_size, shuffle=True)\n",
    "test_iter = Data.DataLoader(test_set, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 bidirectional, labels, use_gpu, **kwargs):\n",
    "        super(SentimentNet, self).__init__(**kwargs)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        self.use_gpu = use_gpu\n",
    "        self.bidirectional = bidirectional\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_size)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.encoder = nn.LSTM(input_size=embed_size, hidden_size=self.num_hiddens,\n",
    "                               num_layers=num_layers, bidirectional=self.bidirectional,\n",
    "                               dropout=0)\n",
    "        if self.bidirectional:\n",
    "            self.decoder = nn.Linear(num_hiddens * 4, labels)\n",
    "        else:\n",
    "            self.decoder = nn.Linear(num_hiddens * 2, labels)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        states, hidden = self.encoder(embeddings.permute([1, 0, 2]))\n",
    "        encoding = torch.cat([states[0], states[-1]], dim=1)\n",
    "        outputs = self.decoder(encoding)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "embed_size = 300\n",
    "num_hiddens = 100\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "batch_size = 64\n",
    "labels = 6\n",
    "lr = 0.005\n",
    "#device = torch.device('cuda:0')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "use_gpu = True\n",
    "\n",
    "net = SentimentNet(vocab_size=(len(vocab)+1), embed_size=embed_size,\n",
    "                   num_hiddens=num_hiddens, num_layers=num_layers,\n",
    "                   bidirectional=bidirectional,\n",
    "                   labels=labels, use_gpu=use_gpu)\n",
    "net.to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_embedding(words, pretrained_vocab):\n",
    "    \"\"\"从预训练好的vocab中提取出words对应的词向量\"\"\"\n",
    "    embed = torch.zeros(len(words), pretrained_vocab.vectors[0].shape[0]) # 初始化为0\n",
    "    oov_count = 0 # out of vocabulary\n",
    "    for i, word in enumerate(words):\n",
    "        try:\n",
    "            idx = pretrained_vocab.stoi[word]\n",
    "            embed[i, :] = pretrained_vocab.vectors[idx]\n",
    "        except KeyError:\n",
    "            oov_count += 0\n",
    "    if oov_count > 0:\n",
    "        print(\"There are %d oov words.\")\n",
    "    return embed\n",
    "\n",
    "net.embedding.weight.data.copy_(\n",
    "    load_pretrained_embedding(vocab.itos, glove_vocab))\n",
    "net.embedding.weight.requires_grad = False # 直接加载预训练好的, 所以不需要更新它\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train loss: 1.7418, train acc: 0.27, test loss: 1.7544, test acc: 0.22, time: 5.15\n",
      "epoch: 1, train loss: 1.7311, train acc: 0.26, test loss: 1.7474, test acc: 0.22, time: 5.00\n",
      "epoch: 2, train loss: 1.7190, train acc: 0.27, test loss: 1.7424, test acc: 0.22, time: 5.12\n",
      "epoch: 3, train loss: 1.7106, train acc: 0.27, test loss: 1.7396, test acc: 0.22, time: 4.92\n",
      "epoch: 4, train loss: 1.7031, train acc: 0.26, test loss: 1.7377, test acc: 0.22, time: 4.92\n",
      "epoch: 5, train loss: 1.6935, train acc: 0.27, test loss: 1.7361, test acc: 0.22, time: 4.89\n",
      "epoch: 6, train loss: 1.6951, train acc: 0.26, test loss: 1.7359, test acc: 0.22, time: 4.96\n",
      "epoch: 7, train loss: 1.6765, train acc: 0.28, test loss: 1.7294, test acc: 0.23, time: 4.95\n",
      "epoch: 8, train loss: 1.6636, train acc: 0.30, test loss: 1.7251, test acc: 0.23, time: 5.03\n",
      "epoch: 9, train loss: 1.6449, train acc: 0.31, test loss: 1.7168, test acc: 0.23, time: 5.25\n",
      "epoch: 10, train loss: 1.6297, train acc: 0.32, test loss: 1.7109, test acc: 0.24, time: 5.04\n",
      "epoch: 11, train loss: 1.6010, train acc: 0.33, test loss: 1.7111, test acc: 0.25, time: 5.22\n",
      "epoch: 12, train loss: 1.5863, train acc: 0.34, test loss: 1.7039, test acc: 0.26, time: 5.11\n",
      "epoch: 13, train loss: 1.5613, train acc: 0.35, test loss: 1.6929, test acc: 0.25, time: 5.15\n",
      "epoch: 14, train loss: 1.5463, train acc: 0.36, test loss: 1.6892, test acc: 0.33, time: 5.09\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "loss_all = []\n",
    "acc_all = []\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    train_loss, test_losses = 0, 0\n",
    "    train_acc, test_acc = 0, 0\n",
    "    \n",
    "    n, m = 0, 0\n",
    "    for feature, label in train_iter:\n",
    "        n += 1\n",
    "        net.zero_grad()\n",
    "        feature = Variable(feature)\n",
    "        label = Variable(label)\n",
    "        score = net(feature)\n",
    "        loss = loss_function(score, label)\n",
    "        #loss = nn.CrossEntropyLoss()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += accuracy_score(torch.argmax(score.cpu().data,\n",
    "                                                 dim=1), label.cpu())\n",
    "        train_loss += loss.cpu().item()\n",
    "    with torch.no_grad():\n",
    "        for test_feature, test_label in test_iter:\n",
    "            m += 1\n",
    "            test_feature = test_feature\n",
    "            test_label = test_label\n",
    "            test_score = net(test_feature)\n",
    "            result.append(test_score)\n",
    "            test_loss = loss_function(test_score, test_label)\n",
    "            test_acc += accuracy_score(torch.argmax(test_score.cpu().data,\n",
    "                                                    dim=1), test_label.cpu())\n",
    "            test_losses += test_loss\n",
    "    end = time.time()\n",
    "    runtime = end - start\n",
    "    loss_all.append(train_loss / n)\n",
    "    acc_all.append(train_acc / n)\n",
    "    print('epoch: %d, train loss: %.4f, train acc: %.2f, test loss: %.4f, test acc: %.2f, time: %.2f' %\n",
    "          (epoch, train_loss / n, train_acc / n, test_losses.data / m, test_acc / m, runtime))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
